---
layout: post
title: Knowledge Graph Papers @ ICLR 2021
authors: Mikhail Galkin, Mila & McGill University
tags: [graph, gnn, knowledge graph, graph ml, graph representation learning]
---

Hi! üëã 
Today we are going to have a look at ICLR 2021 papers focusing on knowledge graphs (KGs), particularly in areas of graph representation learning and NLP. Among [860 accepted papers](https://iclr-conf.medium.com/the-iclr-2021-reviewing-process-and-accepted-papers-7dc65002668e) we highlight 10 particularly interesting and promising works that might influence the field in near future.
This post is be structured as follows:

- [Reasoning in KGs](#reasoning-in-kgs)
- [Temporal Logics and KGs](#temporal-logics-and-kgs)
- [NLP Perspective: Relations, PMI, Entity Linking](#nlp-perspective-relations-pmi-entity-linking)
- [Complex Question Answering: More Modalities](#complex-question-answering-more-modalities)
- [Conclusion](#conclusion)

## Reasoning in KGs

Query embedding and neural query answering are quite hot topics today, and such systems are way more capable in complex reasoning than N+1'th KG embedding model.

Usually, in query embedding, you have to embed a lot of possible combinations of atoms which could easily be 50M points induced by 1-hop, 2-hop, AND, OR, etc queries. That is, starting from a relatively small graph (a subset of Freebase of 270K edges is a typical benchmark) you have to embed orders of magnitude more points. Is it really necessary? ü§®

Surprisingly, no! [Arakelyan, Daza, Minervini, and Cochez](https://openreview.net/pdf?id=Mos9F9kDwkz) show that it is pretty much enough to take any pre-trained KG embedding model (trained only on 1-hop queries in the form `head, relation, ?` ) and decode them in a smart way. The authors propose [CQD (Continuous Query Decomposition)](https://github.com/uclnlp/cqd) with two options: 1) model queries with [t-norms](https://en.wikipedia.org/wiki/T-norm) (continuous optimization); 2) just use a beam search (combinatorial optimization) similar to that of your favourite NLG transformer. That is, you simply traverse the embedding space with beam search and you **don‚Äôt** need all those redundant millions of points. üë®‚Äçüî¨ In the experiments, the beam search strategy performs very well and leaves far behind the previous approaches that do model those millions explicitly. That‚Äôs a neat result and, in my opinion, will be a very strong baseline for all future works in this domain. [Well deserved Outstanding ICLR‚Äô21 Paper award](https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab)! üôå

![Source: Arakelyan, Daza, Minervini, and Cochez]({{ site.url }}/public/images/2022-03-25-kgs/cqd.png)
*Source: [Arakelyan, Daza, Minervini, and Cochez](https://openreview.net/pdf?id=Mos9F9kDwkz)* 

Continuing with rules and reasoning, [Qu, Chen et al](https://openreview.net/pdf?id=tGZu6DlbreV) take another direction and propose [RNNLogic](https://github.com/DeepGraphLearning/RNNLogic) which algorithm is depicted below üëá. RNNLogic employs relational paths that can be mined from the background KG and, hence, generated after some learning procedure. Given a query `head, relation, ?` , we first **generate** a set of rules (sequence of relations parameterized by LSTM, RNN-part of the name comes from here) from which we sample ‚Äòmost plausible‚Äô rules and then send them into the **predictor** to produce scores over possible answers. That is, the generator tries to predict better and better rules pruning the search space for the predictor. The predictor can be parameterized by entity and relation embeddings akin to [RotatE](https://openreview.net/pdf?id=HkgEQnRqYQ), which shows observable improvements in the experiments. During inference, RNNLogic not only predicts a target entity of a query, but also supports it with a bunch of relevant rules which positively impacts explainability, a common pitfall of embeddings-only algorithms.

![RNNLogic. Source: Qu, Chen et al]({{ site.url }}/public/images/2022-03-25-kgs/rnnlogic.png)
*RNNLogic. Source: [Qu, Chen et al](https://openreview.net/pdf?id=tGZu6DlbreV)* 

## Temporal Logics and KGs

Inthe temporal setup, we add a time dimension to our KG. That is, now we have timestamped quadruples `(head, relation, tail, time)` as data points and hence our queries are `(head, relation, ?, time)`. In other words, a model has to take into account when a particular relation happened.

At ICLR‚Äô21, [Han, Chen, et al](https://arxiv.org/pdf/2012.15537.pdf) propose [xERTE](https://github.com/TemporalKGTeam/xERTE), an attention-based model capable of predicting future links üßô‚Äç‚ôÇÔ∏è. The crux of xERTE is iterative subgraph expansion around `head` and this expansion keeps track of seen timestamps so that _prior_ links do not have access to the _posterior_ ones. In some sense, it can be seen as a temporal extension of [GraIL (ICML‚Äô20)](https://arxiv.org/pdf/1911.06962.pdf). Each node embedding is obtained through a concatenation of entity embedding and _time embedding_ (which happens to be a d-dimensional vector of cosines of varying frequencies). Then, in _L_ steps, usually less than 4, xERTE computes attention over the neighbours, prunes the subgraph to retain only the most probable nodes, and yields a distribution of attention scores over candidates (üñº üëá). Thanks to the iterative nature, xERTE can visualize reasoning paths of ranked predictions which was well appreciated by 50+ participants of the user study!

![xERTE. Source: Han, Chen, et al]({{ site.url }}/public/images/2022-03-25-kgs/xerte.png)
*xERTE. Source: [Han, Chen, et al](https://arxiv.org/pdf/2012.15537.pdf)* 

I‚Äôd also put in this section a very interesting work by [Hahn et al](https://openreview.net/pdf?id=dOcQK-f4byz) on learning to solve [Linear Temporal Logic (LTL)](https://en.wikipedia.org/wiki/Linear_temporal_logic) formulas which are widely used in formal verification. LTL is based on propositional logic with temporal operators _Next_ (some formula holds in the next position of a sequence), _Until_ (some formula _f_ holds until _g_ holds), _‚Äúevery point in time‚Äù_, and _‚Äúfuture point in time‚Äù_. The formulas might look like this, i.e., they are pretty long sequences of atoms and operators:

![LTL examples. Source: Hahn et al]({{ site.url }}/public/images/2022-03-25-kgs/ltl.png)
*LTL examples. Source: [Hahn et al](https://openreview.net/pdf?id=dOcQK-f4byz)* 

The authors pose the task of predicting a solution of LTL formulas by generating a satisfiable _trace_ üë£.
What do we do with sequences? Put them into the Transformer, of course.

Enhanced with [tree-based positional encodings](https://www.microsoft.com/en-us/research/uploads/prod/2019/10/shiv_quirk_neurips_2019.pdf) of such long formulas, the authors find that even a relatively small Transformer (8 layers, 8 heads, 1024 FC size) yields surprisingly good results, accurate both semantically and syntactically. Since verifying logical formulas is much simpler than finding them (usually log vs polynomial), the Transformer could generate plausible solutions which then can be verified by non-neural solvers. Furthermore, the authors observe that the Transformer can generalize to the semantics of LTL and perform well on larger/longer formulas compared to training formulas!

## NLP Perspective: Relations, PMI, Entity Linking

There is a good amount of NLP-related research involving KGs this year.

First, [Allen, Bala≈æeviƒá, and Hospedales](https://openreview.net/pdf?id=gLWj29369lW) study the nature of learnable relation embedding in KGs from the [PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (pointwise mutual information) point of view. Back in 2014, [Levy and Goldberg](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf) showed (in their very influential paper) that learning word2vec implicitly factorizes a PMI matrix of words co-occurrences. Then it was shown that we could extract particular semantic concepts like **relatedness, paraphrase, similarity, and analogy** from that PMI matrix. Can we draw some parallels and observe such patterns in learnable KG relations?

Turns out yes! The authors identified 3 possible categories of relations: 1) those which signal about the **relatedness** of two nodes (eg, verb_group relation in Wordnet); 2) those which exhibit **specialization** (hyponym-hypernym); 3) most common **context shift** (eg, meronym). Furthermore, the matrices of **relatedness-type** relations tend to be more symmetric, and eigenvalues/norms of relation matrices/vectors indicate the strength of relatedness. The authors then demonstrate that multiplicative models like DistMult or TuckER better capture such relatedness relation types in KGs. üèÉ‚Äç‚ôÄ Chasing SOTA, current KG embedding literature lacks deep analysis of what is actually learned there, and it‚Äôs great to see such a long-needed qualitative study!

![Source: Allen, Bala≈æeviƒá, and Hospedales]({{ site.url }}/public/images/2022-03-25-kgs/pmi.png)
*Source: [Allen, Bala≈æeviƒá, and Hospedales](https://openreview.net/pdf?id=gLWj29369lW)* 

[Ding, Wang, et al](https://openreview.net/pdf?id=aCgLmfhIy_f) also present a work focusing on relations, but this time in a context of relation extraction from raw texts and learning relation _prototypes_. That is, instead of learning to distinguish hundreds of unique relations (where some of them might be semantically similar), we‚Äôd rather learn a smaller set of **centroids/prototypes** which would group similar relations together on a manifold ‚Äî the authors propose a unit sphere (see illustration). For pre-training, the authors use weak labels from Wikidata using their relations together with mapped entities from Wikipedia. The resulting approach performs particularly well in zero- and few-shot scenario with up to 10% of absolute improvement üí™.

![Relation prototypes. Source: Ding, Wang, et al]({{ site.url }}/public/images/2022-03-25-kgs/rprot.png)
*Relation prototypes. Source: [Ding, Wang, et al](https://openreview.net/pdf?id=aCgLmfhIy_f)* 

Moving towards entities, [De Cao et al](https://openreview.net/pdf?id=5k8F6UU39V) propose another look onto the entity linking task. Usually, in retrievers and entity linkers such as [DPR](https://huggingface.co/transformers/model_doc/dpr.html) or [BLINK](https://github.com/facebookresearch/BLINK), you have to keep in memory the whole index of named entities where lots of entities have certain tokens in common, eg., _‚ÄúLeonardo DiCaprio‚Äù_, _‚ÄúLeonardo da Vinci‚Äù_, _‚ÄúNew York‚Äù_, _‚ÄúNew Jersey‚Äù_, etc.

Of course, in large knowledge bases of millions of entities this leads to a large memory consumption and a necessity to have hard negative samples during training to be able to distinguish between _‚ÄúNew York‚Äù_ and _‚ÄúNew Jersey‚Äù_. Instead, the authors propose [GENRE](https://github.com/facebookresearch/GENRE) (generative entity retrieval) to **generate** entity names autoregressively (token by token) given a context (check out an awesome illustration below üëá). As a backbone, the authors use [BART](https://huggingface.co/transformers/model_doc/bart.html) to fine-tune on generating entity names. The inference process using a beam search is a bit more cumbersome: since we want to prune impossible combinations (eg, not sampling ‚ÄúJersey‚Äù after ‚ÄúLeonardo‚Äù), the authors build a [prefix tree (trie)](https://en.wikipedia.org/wiki/Trie) which encodes 6M Wikipedia titles in a decent 600 MB index. GENRE is also parameter-efficient üèã : while DPR or BLINK require 30‚Äì70GB of memory and 6‚Äì15B (billion) parameters, GENRE only requires 2GB and 17M (million) parameters!

![Generating entity names token by token. Source: GENRE github repo]({{ site.url }}/public/images/2022-03-25-kgs/genre.gif)
*Generating entity names token by token. Source: [GENRE github repo](https://github.com/facebookresearch/GENRE)* 

By the way, a multilingual version, [mGENRE](https://arxiv.org/pdf/2103.12528.pdf), has been published and released either üòâ

## Complex Question Answering: More Modalities

Research on open-domain QA often employs graph structures [between documents as reasoning paths](https://openreview.net/pdf?id=SJgVHkrYDH) (whereas KG-based QA directly traverses a background KG). Open-domain QA immediately benefits from enormously large LMs and recent dense retrieval techniques, that‚Äôs why more efforts from big labs are put along this dimension.

First, [Xiong, Li, et al](https://openreview.net/pdf?id=EMHoBG0avc1) extend the idea of [Dense Passage Retriever](https://huggingface.co/transformers/model_doc/dpr.html) to the multi-hop setup where complex questions are answered iteratively step by step. During training, you‚Äôd feed [MDR (Multi-hop Dense Retriever)](https://github.com/facebookresearch/multihop_dense_retrieval) with a question and previously extracted passages together with positive and negative samples of possible passages, so it‚Äôs pretty close to the original DPR. At inference (check the illustration below), the authors apply beam search and [MIPS](https://en.wikipedia.org/wiki/Inner_product_space) to generate top-K passages, score them, and prepend best candidates to the query at the next iteration. Pretty much all existing multi-hop QA datasets can be solved in 2‚Äì3 steps, so it‚Äôs not a big burden on the system.

üß™ Experiments show that _the graph structure is not necessary here_. That is, you could omit mining and traversing links between paragraphs and resort to the dense index alone to get even better prediction quality! On average, MDR is **5‚Äì20** absolute points better and **10x** faster than its contenders. Besides, does the chosen approach (beam search over pre-trained index) resemble conceptually CQD for KG query answering from the first section? üòâ

![MDR intuition. Source: Xiong, Li, et al]({{ site.url }}/public/images/2022-03-25-kgs/mdr.png)
*MDR intuition. Source: [Xiong, Li, et al](https://openreview.net/pdf?id=EMHoBG0avc1)* 

While MDR focuses on passages of pure text (being them extracted from Wikipedia or other text-only sources), a continuing trend is to cover more sources beyond flat texts. To this end, [Chen et al](https://openreview.net/pdf?id=MmCRswl1UYl) study the problem of complex QA over both _tabular_ and _textual_ data and construct a new [OTT-QA dataset](https://github.com/wenhuchen/OTT-QA) (Open Table-and-Text Question Answering). The authors suggest an elegant solution of linearizing tables into **table segments** to be put into the transformer: split a table in rows and prepend to each row some common information about the whole table (eg, title, min/max values). Doing so, 400k original tables were transformed into 5M segments which is a difficult enough task for the table retriever. Conversely, the proposed model has to learn to retrieve both relevant segments and text passages.

In the experiments, the authors find that a traditional BERT-based iterative retriever-reader works quite poorly (10% F1 score) and instead propose to group connected passages and table segments together into **fused blocks**. Such an early fusion is achieved through entity linking of cells contents to textual mentions. Stacking all the goodies (fusion + long-range transformers + improved reader) the quality increases to 32% F1 üí™. In the last sentence of the paper the authors ask: can we employ even more modalities into the QA..?

![Source: Chen et al]({{ site.url }}/public/images/2022-03-25-kgs/ottqa.png) 
*Source: [Chen et al](https://openreview.net/pdf?id=MmCRswl1UYl)*

‚Ä¶ And [Talmor, Yoran, Catav, Lahav et al](https://openreview.net/pdf?id=ee6W5UgQLa) answer this question right away in their work building [MultiModalQA](https://allenai.github.io/multimodalqa/)! A new dataset poses a multi-hop cross-modal reasoning objective over text üìö, tables üìä, and images üñº. Cross-modal here means that at least one hop in a question implies querying another modality. In the example, a question consists of 3 hops, and each hop can be answered by a relevant source in its own modality. Overall, the dataset consists of ~30K QA-pairs spanning across 16 different compositional templates (eg, combining answers from one table and one image, a template will indicate which modalities have to be queried).

üë©‚Äçüî¨ Empirically, the authors show that one-modality baselines yield only about 18 F1 points, while a fused model (called _ImplicitDecomp_) which derives modalities from classified templates returns ~56 F1 üìà. Text and table QA modules use RoBERTa-Large while the visual QA module employs [VILBERT-MT](https://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_12-in-1_Multi-Task_Vision_and_Language_Representation_Learning_CVPR_2020_paper.pdf). It‚Äôs still far from a human score of 91 F1, so take a note ‚Äî there is a new unsaturated benchmark üòâ.

![Source: Talmor, Yoran, Catav, Lahav et al]({{ site.url }}/public/images/2022-03-25-kgs/mmqa.png) 
*Source: [Talmor, Yoran, Catav, Lahav et al](https://openreview.net/pdf?id=ee6W5UgQLa)*

## Conclusion

That‚Äôs all for today! This conference year, we have seen a lot of examples of out-of-the-box thinking (eg, in KG reasoning, entity linking, drawing parallels to similar domains) which lead to actually cool results - and I would encourage you to try out the same! Maybe that unusual idea you‚Äôve been dismissing recently is actually worth trying?